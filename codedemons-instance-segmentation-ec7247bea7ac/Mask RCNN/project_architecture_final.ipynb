{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_architecture_final",
      "provenance": [],
      "collapsed_sections": [
        "gsVrRVGvHXcS",
        "_uXkv9hIIB_D",
        "UpdGId_JIcO-",
        "qWAbT4rJIszi",
        "LL47HBe5JWOy",
        "7ZLI21tYKlHr",
        "PvOwufLEMViE",
        "tQWuBh91JW0s",
        "aKXN-KcYUWuR",
        "3yU9au-b22tY",
        "VwV9z5m9Vxlm",
        "VH9M3nzPWhe9",
        "QcpynsudYwnk",
        "eYdmIQYEXb9K"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsVrRVGvHXcS",
        "colab_type": "text"
      },
      "source": [
        "# **IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkgxJ8QNHTTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from os import listdir\n",
        "from skimage.io import imread\n",
        "import json\n",
        "import math\n",
        "from skimage.draw import polygon as pls\n",
        "import keras.layers as KL\n",
        "import keras.models as KM\n",
        "import keras.engine as KE\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "from keras.layers import BatchNormalization as BatchNorm\n",
        "import skimage.io\n",
        "import scipy\n",
        "import skimage.transform\n",
        "import urllib.request\n",
        "import shutil\n",
        "import warnings\n",
        "from distutils.version import LooseVersion\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uXkv9hIIB_D",
        "colab_type": "text"
      },
      "source": [
        "# **ADDING FILES TO COLAB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c87jK7gwSIBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIBT4EA_ICYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/IDD.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpdGId_JIcO-",
        "colab_type": "text"
      },
      "source": [
        "# **DATASET CLASS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX9ZQ66fIcrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class data(object):\n",
        "  ''' Dataset class to store the class and image info \n",
        " Author - Joseph Arul Raj '''\n",
        "\n",
        "  def __init__(self):\n",
        "    self.image_info = []\n",
        "    self.class_info = [{\"source\":\"Background\",\"id\": 0, \"name\": \"BG\"}]\n",
        "\n",
        "  def add_class(self,source,class_id, class_name):\n",
        "      unwanted=['sky','building','billboard','road']\n",
        "      for info in self.class_info:\n",
        "          # if info['name'] == class_name  or unwanted:\n",
        "          if class_name== info['name'] :\n",
        "              return False\n",
        "          if class_name in unwanted:\n",
        "              return False\n",
        "      self.class_info.append({\"source\":source,\n",
        "                              \"id\":class_id,\n",
        "                              \"name\":class_name,})\n",
        "      return True\n",
        "\n",
        "  def add_image(self,image_id, path, **kwargs):\n",
        "      image_info = {\n",
        "          \"id\": image_id,\n",
        "          \"path\": path,\n",
        "      }\n",
        "      image_info.update(kwargs)\n",
        "      self.image_info.append(image_info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWAbT4rJIszi",
        "colab_type": "text"
      },
      "source": [
        "# **LOADING THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amygDrjNItIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data_dir=\"/content/IDD\"\n",
        "\n",
        "class anno(data):\n",
        "'''Class for data preparation.\n",
        "load_data is to load the annotations from json ffile and to retrive the polygon coordinates for the masks in an image\n",
        "load_mask is to generate a mask from the obtained coordinates\n",
        "extract_bboxes is to create a bounding box for each of the generated masks.\n",
        "Author - Joseph Arul Raj '''\n",
        "  def load_data(self,data_dir):\n",
        "    unwanted=['sky','building','billboard','road']\n",
        "    img_dir=data_dir + \"/img/\"\n",
        "    ann_dir=data_dir + \"/ann/\"\n",
        "    c_id=1\n",
        "    pol=[]\n",
        "    for files in listdir(ann_dir):\n",
        "      pol1=[]\n",
        "      ann_path=ann_dir + files\n",
        "      ann_file=json.load(open(ann_path))\n",
        "      annotations=list(ann_file.values())[2]\n",
        "      for i in range(len(annotations)):\n",
        "        ann=annotations[i]\n",
        "        name=ann['label']\n",
        "        added=self.add_class(source=files[:6],class_id=c_id,class_name=name)\n",
        "        if added:\n",
        "          c_id +=1\n",
        "        # return added\n",
        "      for i in range(len(annotations)):\n",
        "        cname=annotations[i]['label']\n",
        "        if cname not in unwanted:\n",
        "          pol1.append(annotations[i]['polygon'])\n",
        "      pol.append(pol1)\n",
        "    pol2=[]\n",
        "    for i in range(len(pol)):\n",
        "      pol3=[]\n",
        "      for j in range(len(pol[i])):\n",
        "        pol4={'x':[],\n",
        "               'y':[]}\n",
        "        for k in range(len(pol[i][j])):\n",
        "          pol4['x'].append(pol[i][j][k][0])\n",
        "          pol4['y'].append(pol[i][j][k][1])\n",
        "        pol3.append(pol4)\n",
        "      pol2.append(pol3)\n",
        "\n",
        "    \n",
        "    for f_name in listdir(img_dir):\n",
        "      image_id=f_name[:6]\n",
        "      img_path=img_dir+f_name\n",
        "      image=imread(img_path)\n",
        "      ann_path=ann_dir+image_id+'_gtFine_polygons.json'\n",
        "      height, width = image.shape[:2]\n",
        "      self.add_image( image_id=image_id, path=img_path, annotation=ann_path,width=width, height=height)\n",
        "\n",
        "    return pol2\n",
        "\n",
        "  def load_mask(self,pol):\n",
        "    masks=[]\n",
        "    for k in range(len(pol)):\n",
        "      mask=np.zeros((self.image_info[k]['height'],self.image_info[k]['width'],len(pol[k])))\n",
        "      for i , p in enumerate(pol[k]):\n",
        "        rr,cc=pls(p['y'],p['x'])\n",
        "        mask[rr,cc,i]=1\n",
        "      masks.append(mask)\n",
        "    return masks\n",
        "  def extract_bboxes(self,masks):\n",
        "    box = np.zeros([masks.shape[-1], 4], dtype=np.int32)\n",
        "    for j in range(masks.shape[-1]):\n",
        "        m = masks[:, :, j]\n",
        "        # Bounding box.\n",
        "        horizontal_indicies = np.where(np.any(m, axis=0))[0]\n",
        "        vertical_indicies = np.where(np.any(m, axis=1))[0]\n",
        "        if horizontal_indicies.shape[0]:\n",
        "            x1, x2 = horizontal_indicies[[0, -1]]\n",
        "            y1, y2 = vertical_indicies[[0, -1]]\n",
        "            x2 += 1\n",
        "            y2 += 1\n",
        "        else:\n",
        "            x1, x2, y1, y2 = 0, 0, 0, 0\n",
        "        box[j] = np.array([y1, x1, y2, x2])\n",
        "    return box\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2t03Sk0KHEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Verify the loaded data '''\n",
        "train=anno()\n",
        "da=train.load_data(data_dir)\n",
        "mask=train.load_mask(da)\n",
        "bbox=train.extract_bboxes(mask)\n",
        "img_dir=data_dir + \"/img/\"\n",
        "images=np.zeros([2,1080,1920,3])\n",
        "for filename , i in zip(listdir(img_dir),range(2)):\n",
        "  im=img_dir+filename\n",
        "  images[i]=img_to_array(load_img(im,target_size=(1080,1920)))\n",
        "# plot and see the masks in an image\n",
        "plt.imshow(images[1]/255)\n",
        "plt.imshow(mask[1][:, :, 79], alpha=0.5)\n",
        "plt.imshow(mask[1][:, :, 95], alpha=0.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL47HBe5JWOy",
        "colab_type": "text"
      },
      "source": [
        "# **ANCHORS GENERATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WFr3ut3JWhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Generate anchor on the given feature map\n",
        "Author - Joseph Arul Raj '''\n",
        "\n",
        "\n",
        "def norm_boxes_graph(boxes, shape):\n",
        "    h, w = tf.split(tf.cast(shape, tf.float32), 2)\n",
        "    scale = tf.concat([h, w, h, w], axis=-1) - tf.constant(1.0)\n",
        "    shift = tf.constant([0., 0., 1., 1.])\n",
        "    return tf.divide(boxes - shift, scale)\n",
        "\n",
        "def compute_backbone_shapes(image_shape,feature_strides):\n",
        "  return np.array(\n",
        "        [[int(math.ceil(image_shape[0] / stride)),\n",
        "            int(math.ceil(image_shape[1] / stride))]\n",
        "            for stride in feature_strides])\n",
        "def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride):\n",
        "    # Get all combinations of scales and ratios\n",
        "    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n",
        "    scales = scales.flatten()\n",
        "    ratios = ratios.flatten()\n",
        "\n",
        "    # Enumerate heights and widths from scales and ratios\n",
        "    heights = scales / np.sqrt(ratios)\n",
        "    widths = scales * np.sqrt(ratios)\n",
        "\n",
        "    # Enumerate shifts in feature space\n",
        "    shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n",
        "    shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
        "    shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n",
        "\n",
        "    # Enumerate combinations of shifts, widths, and heights\n",
        "    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n",
        "    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n",
        "\n",
        "    # Reshape to get a list of (y, x) and a list of (h, w)\n",
        "    box_centers = np.stack(\n",
        "        [box_centers_y, box_centers_x], axis=2).reshape([-1, 2])\n",
        "    box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2])\n",
        "\n",
        "    # Convert to corner coordinates (y1, x1, y2, x2)\n",
        "    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n",
        "                            box_centers + 0.5 * box_sizes], axis=1)\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides,\n",
        "                             anchor_stride):\n",
        "   # Anchors\n",
        "    # [anchor_count, (y1, x1, y2, x2)]\n",
        "    anchors = []\n",
        "    for i in range(len(scales)):\n",
        "        anchors.append(generate_anchors(scales[i], ratios, feature_shapes[i],\n",
        "                                        feature_strides[i], anchor_stride))\n",
        "    return np.concatenate(anchors, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZLI21tYKlHr",
        "colab_type": "text"
      },
      "source": [
        "# **BACKBONE CNN -RESNET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umC6e0uyKlV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Resnet graph for the backbone cnn\n",
        "author -Asad Tanveer'''\n",
        "\n",
        "# The identity_block is the block that has no conv layer at shortcut\n",
        "def identity_block(input_tensor, kernel_size, filters,train_bn=True):\n",
        "   nb_filter1, nb_filter2, nb_filter3 = filters\n",
        "   x = KL.Conv2D(nb_filter1, (1, 1),use_bias=True)(input_tensor)\n",
        "   x = KL.BatchNormalization()(x,training=True)\n",
        "   x = KL.Activation('relu')(x)\n",
        "\n",
        "   x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same',use_bias=True)(x)\n",
        "   x = KL.BatchNormalization()(x,training=True)\n",
        "   x = KL.Activation('relu')(x)\n",
        "\n",
        "   x = KL.Conv2D(nb_filter3, (1, 1),use_bias=True)(x)\n",
        "   x = KL.BatchNormalization()(x,training=True)\n",
        "\n",
        "   x = KL.Add()([x, input_tensor])\n",
        "   x = KL.Activation('relu')(x)\n",
        "   return x\n",
        "# conv_block is the block that has a conv layer at shortcut\n",
        "def conv_block(input_tensor, kernel_size, filters,strides=(2, 2), use_bias=True, train_bn=True):\n",
        "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
        "    x = KL.Conv2D(nb_filter1, (1, 1), strides=strides, use_bias=use_bias)(input_tensor)\n",
        "    x = KL.BatchNormalization()(x, training=train_bn)\n",
        "    x = KL.Activation('relu')(x)\n",
        "\n",
        "    x = KL.Conv2D(nb_filter2, (kernel_size, kernel_size), padding='same', use_bias=use_bias)(x)\n",
        "    x = KL.BatchNormalization()(x, training=train_bn)\n",
        "    x = KL.Activation('relu')(x)\n",
        "\n",
        "    x = KL.Conv2D(nb_filter3, (1, 1), use_bias=use_bias)(x)\n",
        "    x = KL.BatchNormalization()(x, training=train_bn)\n",
        "\n",
        "    shortcut = KL.Conv2D(nb_filter3, (1, 1), strides=strides, use_bias=use_bias)(input_tensor)\n",
        "    shortcut = KL.BatchNormalization()(shortcut, training=train_bn)\n",
        "\n",
        "    x = KL.Add()([x, shortcut])\n",
        "    x = KL.Activation('relu')(x)\n",
        "    return x\n",
        "# Build a ResNet graph.\n",
        "def resnet_graph(input_image, train_bn=True):\n",
        "    # Stage 1\n",
        "    x = KL.ZeroPadding2D((3, 3))(input_image)\n",
        "    x = KL.Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)(x)\n",
        "    x = KL.BatchNormalization()(x, training=train_bn)\n",
        "    x = KL.Activation('relu')(x)\n",
        "    C1 = x = KL.MaxPooling2D((3, 3), strides=(2, 2), padding=\"same\")(x)\n",
        "    # Stage 2\n",
        "    x = conv_block(x, 3, [64, 64, 256],strides=(1, 1), train_bn=train_bn)\n",
        "    x = identity_block(x, 3, [64, 64, 256],train_bn=train_bn)\n",
        "    C2 = x = identity_block(x, 3, [64, 64, 256], train_bn=train_bn)\n",
        "    # Stage 3\n",
        "    x = conv_block(x, 3, [128, 128, 512], train_bn=train_bn)\n",
        "    x = identity_block(x, 3, [128, 128, 512], train_bn=train_bn)\n",
        "    x = identity_block(x, 3, [128, 128, 512], train_bn=train_bn)\n",
        "    C3 = x = identity_block(x, 3, [128, 128, 512], train_bn=train_bn)\n",
        "    # Stage 4\n",
        "    x = conv_block(x, 3, [256, 256, 1024], train_bn=train_bn)\n",
        "    for i in range(5):\n",
        "        x = identity_block(x, 3, [256, 256, 1024], train_bn=train_bn)\n",
        "    C4 = x\n",
        "    # Stage 5\n",
        "    x = conv_block(x, 3, [512, 512, 2048], train_bn=train_bn)\n",
        "    x = identity_block(x, 3, [512, 512, 2048], train_bn=train_bn)\n",
        "    C5 = x = identity_block(x, 3, [512, 512, 2048], train_bn=train_bn)\n",
        "    return [C1, C2, C3, C4, C5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvOwufLEMViE",
        "colab_type": "text"
      },
      "source": [
        "## **CONSTRUCT A FEATURE PYRAMID NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJNZQUZXMg4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''The FPN that connects the backbone cnn with RPN\n",
        "author - Joseph Arul Raj '''\n",
        "\n",
        "# Build dummy imputs\n",
        "img_shape=[1024,1024,3]\n",
        "input_image=KL.Input(shape=[None,None,3])\n",
        "input_rpn_bbox = KL.Input(shape=[None, 4], name=\"input_rpn_bbox\", dtype=tf.float32)\n",
        "input_gt_boxes = KL.Input(shape=[None, 4], name=\"input_gt_boxes\", dtype=tf.float32)\n",
        "gt_boxes = KL.Lambda(lambda x: norm_boxes_graph(x, K.shape(input_image)[1:3]))(input_gt_boxes)\n",
        "input_gt_masks = KL.Input(shape=[img_shape[0], img_shape[1], 100],name=\"input_gt_masks\", dtype=bool)\n",
        "\n",
        "# Get the feature maps from Resnet graph\n",
        "_, C2, C3, C4, C5 = resnet_graph(input_image)\n",
        "\n",
        "# FPN graph\n",
        "\n",
        "P5 = KL.Conv2D(256, (1, 1), name='fpn_c5p5')(C5)\n",
        "P4 = KL.Add(name=\"fpn_p4add\")([\n",
        "    KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5),\n",
        "    KL.Conv2D(256, (1, 1), name='fpn_c4p4')(C4)])\n",
        "P3 = KL.Add(name=\"fpn_p3add\")([\n",
        "    KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4),\n",
        "    KL.Conv2D(256, (1, 1), name='fpn_c3p3')(C3)])\n",
        "P2 = KL.Add(name=\"fpn_p2add\")([\n",
        "    KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3),\n",
        "    KL.Conv2D(256, (1, 1), name='fpn_c2p2')(C2)])\n",
        "# Attach 3x3 conv to all P layers to get the final feature maps.\n",
        "P2 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p2\")(P2)\n",
        "P3 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p3\")(P3)\n",
        "P4 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p4\")(P4)\n",
        "P5 = KL.Conv2D(256, (3, 3), padding=\"SAME\", name=\"fpn_p5\")(P5)\n",
        "# P6 is used for the 5th anchor scale in RPN. Generated by\n",
        "# subsampling from P5 with stride of 2.\n",
        "P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5)\n",
        "\n",
        "rpn_feature_maps = [P2, P3, P4, P5, P6]\n",
        "mrcnn_feature_maps = [P2, P3, P4, P5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQWuBh91JW0s",
        "colab_type": "text"
      },
      "source": [
        "# **RPN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5H-v8ubNWAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Region propasal graph - generates two outputs. One containing the object score and the other containing the bounding box coordinates\n",
        "author - Asad Tanveer'''\n",
        "\n",
        "def rpn_graph(feature_map, anchors_per_location, anchor_stride):\n",
        "    shared = KL.Conv2D(512, (3, 3), padding='same', activation='relu',\n",
        "                       strides=anchor_stride,\n",
        "                       name='rpn_conv_shared')(feature_map)\n",
        "\n",
        "    # Anchor Score. [batch, height, width, anchors per location * 2].\n",
        "    x = KL.Conv2D(2 * anchors_per_location, (1, 1), padding='valid',\n",
        "                  activation='linear', name='rpn_class_raw')(shared)\n",
        "\n",
        "    # Reshape to [batch, anchors, 2]\n",
        "    rpn_class_logits = KL.Lambda(\n",
        "        lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 2]))(x)\n",
        "\n",
        "    # Softmax on last dimension of BG/FG.\n",
        "    rpn_probs = KL.Activation(\n",
        "        \"softmax\", name=\"rpn_class_xxx\")(rpn_class_logits)\n",
        "\n",
        "    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n",
        "    # where depth is [x, y, log(w), log(h)]\n",
        "    x = KL.Conv2D(anchors_per_location * 4, (1, 1), padding=\"valid\",\n",
        "                  activation='linear', name='rpn_bbox_pred')(shared)\n",
        "\n",
        "    # Reshape to [batch, anchors, 4]\n",
        "    rpn_bbox = KL.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], -1, 4]))(x)\n",
        "\n",
        "    return [rpn_class_logits, rpn_probs, rpn_bbox]\n",
        "\n",
        "\n",
        "input_feature_map = KL.Input(shape=[None, None, 256])\n",
        "outputs = rpn_graph(input_feature_map, 3, 1)\n",
        "rpn= KM.Model([input_feature_map], outputs, name=\"rpn_model\")\n",
        "# Map the feature maps generated from the FPN with the RPN graph\n",
        "layer_outputs = []  # list of lists\n",
        "for p in rpn_feature_maps:\n",
        "    layer_outputs.append(rpn([p]))\n",
        "output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"]\n",
        "outputs = list(zip(*layer_outputs))\n",
        "outputs = [KL.Concatenate(axis=1, name=n)(list(o))\n",
        "            for o, n in zip(outputs, output_names)]\n",
        "\n",
        "rpn_class_logits, rpn_class, rpn_bbox = outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKXN-KcYUWuR",
        "colab_type": "text"
      },
      "source": [
        "# **PROPOSAL LAYER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Q2Dq_4UaMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Proposal layer is to select 2000 boxes from the generated 200k boxes\n",
        "Author - Joseph Arul Raj '''\n",
        "\n",
        "\n",
        "def apply_box_deltas_graph(boxes, deltas):\n",
        "    \"\"\"Applies the given deltas to the given boxes.\n",
        "    boxes: [N, (y1, x1, y2, x2)] boxes to update\n",
        "    deltas: [N, (dy, dx, log(dh), log(dw))] refinements to apply\n",
        "    \"\"\"\n",
        "    # Convert to y, x, h, w\n",
        "    height = boxes[:, 2] - boxes[:, 0]\n",
        "    width = boxes[:, 3] - boxes[:, 1]\n",
        "    center_y = boxes[:, 0] + 0.5 * height\n",
        "    center_x = boxes[:, 1] + 0.5 * width\n",
        "    # Apply deltas\n",
        "    center_y += deltas[:, 0] * height\n",
        "    center_x += deltas[:, 1] * width\n",
        "    height *= tf.exp(deltas[:, 2])\n",
        "    width *= tf.exp(deltas[:, 3])\n",
        "    # Convert back to y1, x1, y2, x2\n",
        "    y1 = center_y - 0.5 * height\n",
        "    x1 = center_x - 0.5 * width\n",
        "    y2 = y1 + height\n",
        "    x2 = x1 + width\n",
        "    result = tf.stack([y1, x1, y2, x2], axis=1, name=\"apply_box_deltas_out\")\n",
        "    return result\n",
        "\n",
        "# clip the boxes to the image height and width\n",
        "def clip_boxes_graph(boxes, window):\n",
        "    \"\"\"\n",
        "    boxes: [N, (y1, x1, y2, x2)]\n",
        "    window: [4] in the form y1, x1, y2, x2\n",
        "    \"\"\"\n",
        "    # Split\n",
        "    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n",
        "    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n",
        "    # Clip\n",
        "    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n",
        "    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n",
        "    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n",
        "    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n",
        "    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n",
        "    clipped.set_shape((clipped.shape[0], 4))\n",
        "    return clipped\n",
        "\n",
        "class ProposalLayer(KE.Layer):\n",
        "    def __init__(self, proposal_count, nms_threshold, **kwargs):\n",
        "        super(ProposalLayer, self).__init__(**kwargs)\n",
        "        self.proposal_count = proposal_count\n",
        "        self.nms_threshold = nms_threshold\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Box Scores. Use the foreground class confidence. [Batch, num_rois, 1]\n",
        "        sc=inputs[0][:,:,1]\n",
        "        deltas=inputs[1]\n",
        "        anc =inputs[2][1]\n",
        "        RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
        "        deltas = deltas * np.reshape(RPN_BBOX_STD_DEV, [1, 1, 4])\n",
        "        proposals=[]\n",
        "        for i in range(2):\n",
        "          cls_score=sc[i,:]\n",
        "          # select indices for top 6000 boxes based on the object score\n",
        "          ix = tf.math.top_k(cls_score, 6000, sorted=True,name=\"top_anchors\").indices\n",
        "          # select the scores,boxes and anchors from the 6000 indices\n",
        "          bbo=deltas[i,:,:]\n",
        "          sco=tf.gather(cls_score,ix)\n",
        "          bbb=tf.gather(bbo,ix)\n",
        "          aaa=tf.gather(anc,ix)\n",
        "          anbo=apply_box_deltas_graph(aaa,bbb)\n",
        "          window = np.array([0, 0, 1, 1], dtype=np.float32)\n",
        "          cl_bo=clip_boxes_graph(anbo,window)\n",
        "          indices = tf.image.non_max_suppression(cl_bo,sco, self.proposal_count,self.nms_threshold)\n",
        "          proposal = tf.gather(cl_bo, indices)\n",
        "          proposals.append(proposal)\n",
        "        rpn_rois=tf.convert_to_tensor(proposals)\n",
        "        return rpn_rois\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.proposal_count, 4)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yU9au-b22tY",
        "colab_type": "text"
      },
      "source": [
        "# **TARGET DETECTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_fjykBh21UZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To slice the given inputs and pass them as batches\n",
        "\n",
        "def batch_slice(inputs, graph_fn, batch_size, names=None):\n",
        "    if not isinstance(inputs, list):\n",
        "        inputs = [inputs]\n",
        "\n",
        "    outputs = []\n",
        "    for i in range(batch_size):\n",
        "        inputs_slice = [x[i] for x in inputs]\n",
        "        output_slice = graph_fn(*inputs_slice)\n",
        "        if not isinstance(output_slice, (tuple, list)):\n",
        "            output_slice = [output_slice]\n",
        "        outputs.append(output_slice)\n",
        "    # Change outputs from a list of slices where each is\n",
        "    # a list of outputs to a list of outputs and each has\n",
        "    # a list of slices\n",
        "    outputs = list(zip(*outputs))\n",
        "\n",
        "    if names is None:\n",
        "        names = [None] * len(outputs)\n",
        "\n",
        "    result = [tf.stack(o, axis=0, name=n)\n",
        "              for o, n in zip(outputs, names)]\n",
        "    if len(result) == 1:\n",
        "        result = result[0]\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLtmA7wWVYZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Tatget detection layer is to choose 200(both positive and negetive) boxes for training\n",
        "Author - Asad Tanveer'''\n",
        "\n",
        "def box_refinement_graph(box, gt_box):\n",
        "    \"\"\"Compute refinement needed to transform box to gt_box.\n",
        "    box and gt_box are [N, (y1, x1, y2, x2)]\n",
        "    \"\"\"\n",
        "    box = tf.cast(box, tf.float32)\n",
        "    gt_box = tf.cast(gt_box, tf.float32)\n",
        "\n",
        "    height = box[:, 2] - box[:, 0]\n",
        "    width = box[:, 3] - box[:, 1]\n",
        "    center_y = box[:, 0] + 0.5 * height\n",
        "    center_x = box[:, 1] + 0.5 * width\n",
        "\n",
        "    gt_height = gt_box[:, 2] - gt_box[:, 0]\n",
        "    gt_width = gt_box[:, 3] - gt_box[:, 1]\n",
        "    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n",
        "    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n",
        "\n",
        "    dy = (gt_center_y - center_y) / height\n",
        "    dx = (gt_center_x - center_x) / width\n",
        "    dh = tf.log(gt_height / height)\n",
        "    dw = tf.log(gt_width / width)\n",
        "\n",
        "    result = tf.stack([dy, dx, dh, dw], axis=1)\n",
        "    return result\n",
        "\n",
        "def overlaps_graph(boxes1, boxes2):\n",
        "    \"\"\"Computes IoU overlaps between two sets of boxes.\n",
        "    boxes1, boxes2: [N, (y1, x1, y2, x2)].\n",
        "    \"\"\"\n",
        "    # 1. Tile boxes2 and repeat boxes1. This allows us to compare\n",
        "    # every boxes1 against every boxes2 without loops.\n",
        "    # TF doesn't have an equivalent to np.repeat() so simulate it\n",
        "    # using tf.tile() and tf.reshape.\n",
        "    b1 = tf.reshape(tf.tile(tf.expand_dims(boxes1, 1),\n",
        "                            [1, 1, tf.shape(boxes2)[0]]), [-1, 4])\n",
        "    b2 = tf.tile(boxes2, [tf.shape(boxes1)[0], 1])\n",
        "    # 2. Compute intersections\n",
        "    b1_y1, b1_x1, b1_y2, b1_x2 = tf.split(b1, 4, axis=1)\n",
        "    b2_y1, b2_x1, b2_y2, b2_x2 = tf.split(b2, 4, axis=1)\n",
        "    y1 = tf.maximum(b1_y1, b2_y1)\n",
        "    x1 = tf.maximum(b1_x1, b2_x1)\n",
        "    y2 = tf.minimum(b1_y2, b2_y2)\n",
        "    x2 = tf.minimum(b1_x2, b2_x2)\n",
        "    intersection = tf.maximum(x2 - x1, 0) * tf.maximum(y2 - y1, 0)\n",
        "    # 3. Compute unions\n",
        "    b1_area = (b1_y2 - b1_y1) * (b1_x2 - b1_x1)\n",
        "    b2_area = (b2_y2 - b2_y1) * (b2_x2 - b2_x1)\n",
        "    union = b1_area + b2_area - intersection\n",
        "    # 4. Compute IoU and reshape to [boxes1, boxes2]\n",
        "    iou = intersection / union\n",
        "    overlaps = tf.reshape(iou, [tf.shape(boxes1)[0], tf.shape(boxes2)[0]])\n",
        "    return overlaps\n",
        "\n",
        "\n",
        "def detection_targets_graph(proposals, gt_boxes, gt_masks):\n",
        "\n",
        "    # Compute overlaps matrix [proposals, gt_boxes]\n",
        "    overlaps = overlaps_graph(proposals, gt_boxes)\n",
        "\n",
        "    # Compute overlaps with crowd boxes [proposals, crowd_boxes]\n",
        "    # crowd_overlaps = overlaps_graph(proposals, crowd_boxes)\n",
        "    # crowd_iou_max = tf.reduce_max(crowd_overlaps, axis=1)\n",
        "    # no_crowd_bool = (crowd_iou_max < 0.001)\n",
        "\n",
        "    # Determine positive and negative ROIs\n",
        "    roi_iou_max = tf.reduce_max(overlaps, axis=1)\n",
        "    # 1. Positive ROIs are those with >= 0.5 IoU with a GT box\n",
        "    positive_roi_bool = (roi_iou_max >= 0.5)\n",
        "    positive_indices = tf.where(positive_roi_bool)[:, 0]\n",
        "    # 2. Negative ROIs are those with < 0.5 with every GT box. Skip crowds.\n",
        "    negative_indices = tf.where(roi_iou_max < 0.5)[:, 0]\n",
        "\n",
        "    # Subsample ROIs. Aim for 33% positive\n",
        "    # Positive ROIs\n",
        "    positive_count = int(200 *0.33)\n",
        "    positive_indices = tf.random_shuffle(positive_indices)[:positive_count]\n",
        "    positive_count = tf.shape(positive_indices)[0]\n",
        "    # Negative ROIs. Add enough to maintain positive:negative ratio.\n",
        "    r = 1.0 / 0.33\n",
        "    negative_count = tf.cast(r * tf.cast(positive_count, tf.float32), tf.int32) - positive_count\n",
        "    negative_indices = tf.random_shuffle(negative_indices)[:negative_count]\n",
        "    # Gather selected ROIs\n",
        "    positive_rois = tf.gather(proposals, positive_indices)\n",
        "    negative_rois = tf.gather(proposals, negative_indices)\n",
        "\n",
        "    # Assign positive ROIs to GT boxes.\n",
        "    positive_overlaps = tf.gather(overlaps, positive_indices)\n",
        "    roi_gt_box_assignment = tf.cond(\n",
        "        tf.greater(tf.shape(positive_overlaps)[1], 0),\n",
        "        true_fn = lambda: tf.argmax(positive_overlaps, axis=1),\n",
        "        false_fn = lambda: tf.cast(tf.constant([]),tf.int64)\n",
        "    )\n",
        "    roi_gt_boxes = tf.gather(gt_boxes, roi_gt_box_assignment)\n",
        "    # roi_gt_class_ids = tf.gather(gt_class_ids, roi_gt_box_assignment)\n",
        "\n",
        "    # Compute bbox refinement for positive ROIs\n",
        "    BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
        "    deltas = box_refinement_graph(positive_rois, roi_gt_boxes)\n",
        "    deltas /= BBOX_STD_DEV\n",
        "\n",
        "    # Assign positive ROIs to GT masks\n",
        "    # Permute masks to [N, height, width, 1]\n",
        "    transposed_masks = tf.expand_dims(tf.transpose(gt_masks, [2, 0, 1]), -1)\n",
        "    # Pick the right mask for each ROI\n",
        "    roi_masks = tf.gather(transposed_masks, roi_gt_box_assignment)\n",
        "    MASK_SHAPE=[28,28]\n",
        "    # Compute mask targets\n",
        "    boxes = positive_rois\n",
        "    box_ids = tf.range(0, tf.shape(roi_masks)[0])\n",
        "    masks = tf.image.crop_and_resize(tf.cast(roi_masks, tf.float32), boxes,\n",
        "                                     box_ids,\n",
        "                                     MASK_SHAPE)\n",
        "    # Remove the extra dimension from masks.\n",
        "    masks = tf.squeeze(masks, axis=3)\n",
        "\n",
        "    # Threshold mask pixels at 0.5 to have GT masks be 0 or 1 to use with\n",
        "    # binary cross entropy loss.\n",
        "    masks = tf.round(masks)\n",
        "\n",
        "    # Append negative ROIs and pad bbox deltas and masks that\n",
        "    # are not used for negative ROIs with zeros.\n",
        "    rois = tf.concat([positive_rois, negative_rois], axis=0)\n",
        "    N = tf.shape(negative_rois)[0]\n",
        "    P = tf.maximum(200 - tf.shape(rois)[0], 0)\n",
        "    rois = tf.pad(rois, [(0, P), (0, 0)])\n",
        "    roi_gt_boxes = tf.pad(roi_gt_boxes, [(0, N + P), (0, 0)])\n",
        "    # roi_gt_class_ids = tf.pad(roi_gt_class_ids, [(0, N + P)])\n",
        "    deltas = tf.pad(deltas, [(0, N + P), (0, 0)])\n",
        "    masks = tf.pad(masks, [[0, N + P], (0, 0), (0, 0)])\n",
        "\n",
        "    return rois,  deltas, masks\n",
        "\n",
        "\n",
        "class DetectionTargetLayer(KL.Layer):\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        super(DetectionTargetLayer, self).__init__(**kwargs)\n",
        "        # self.config = config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        proposals = inputs[0]\n",
        "        # gt_class_ids = inputs[1]\n",
        "        gt_boxes = inputs[1]\n",
        "        gt_masks = inputs[2]\n",
        "\n",
        "        # Slice the batch and run a graph for each slice\n",
        "        # TODO: Rename target_bbox to target_deltas for clarity\n",
        "        names = [\"rois\", \"target_class_ids\", \"target_bbox\", \"target_mask\"]\n",
        "        outputs = batch_slice([proposals, gt_boxes, gt_masks],lambda  x, y, z: detection_targets_graph( x, y, z),2, names=names)\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return [\n",
        "            (None, 200, 4),  # rois\n",
        "            # (None, 200),  # class_ids\n",
        "            (None, 200, 4),  # deltas\n",
        "            (None, 200,28,28)  # masks\n",
        "        ]\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return [None, None, None, None]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwV9z5m9Vxlm",
        "colab_type": "text"
      },
      "source": [
        "# **GENERATE ROIS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBmcIMtRV2s7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Call the proposal layer to select 2000 boxes\n",
        "\n",
        "rpn_rois = ProposalLayer(proposal_count=2000,nms_threshold=0.7,name=\"ROI\")([rpn_class, rpn_bbox, anchors])\n",
        "\n",
        "# call the detection target layer to choose 200 samples from 2000 boxes\n",
        "\n",
        "rois, target_bbox, target_mask =DetectionTargetLayer(name=\"proposal_targets\")([rpn_rois, gt_boxes, input_gt_masks])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_sgs33pWMir",
        "colab_type": "text"
      },
      "source": [
        "**ROI ALLIGN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMM1vQhCWQgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Applies ROI Allign to the feature maps generated by the FPN \n",
        "Author - Joseph Arul Raj '''\n",
        "\n",
        "def log2_graph(x):\n",
        "    \"\"\"Implementation of Log2. TF doesn't have a native implementation.\"\"\"\n",
        "    return tf.log(x) / tf.log(2.0)\n",
        "\n",
        "\n",
        "class ROIAlign(KL.Layer):\n",
        "    def __init__(self, pool_shape, **kwargs):\n",
        "        super(ROIAlign, self).__init__(**kwargs)\n",
        "        self.pool_shape = tuple(pool_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords\n",
        "        boxes = inputs[0]\n",
        "\n",
        "        # Image meta\n",
        "        # Holds details about the image. See compose_image_meta()\n",
        "        # image_meta = inputs[1]\n",
        "\n",
        "        # Feature Maps. List of feature maps from different level of the\n",
        "        # feature pyramid. Each is [batch, height, width, channels]\n",
        "        feature_maps = inputs[1:]\n",
        "\n",
        "        # Assign each ROI to a level in the pyramid based on the ROI area.\n",
        "        y1, x1, y2, x2 = tf.split(boxes, 4, axis=2)\n",
        "        h = y2 - y1\n",
        "        w = x2 - x1\n",
        "        # Use shape of first image. Images in a batch must have the same size.\n",
        "        image_shape = [1024,1024,3]\n",
        "        # Equation 1 in the Feature Pyramid Networks paper. Account for\n",
        "        # the fact that our coordinates are normalized here.\n",
        "        # e.g. a 224x224 ROI (in pixels) maps to P4\n",
        "        image_area = tf.cast(image_shape[0] * image_shape[1], tf.float32)\n",
        "        roi_level = log2_graph(tf.sqrt(h * w) / (224.0 / tf.sqrt(image_area)))\n",
        "        roi_level = tf.minimum(5, tf.maximum(\n",
        "            2, 4 + tf.cast(tf.round(roi_level), tf.int32)))\n",
        "        roi_level = tf.squeeze(roi_level, 2)\n",
        "\n",
        "        # Loop through levels and apply ROI poinput_gt_masksoling to each. P2 to P5.\n",
        "        pooled = []\n",
        "        box_to_level = []\n",
        "        for i, level in enumerate(range(2, 6)):\n",
        "            ix = tf.where(tf.equal(roi_level, level))\n",
        "            level_boxes = tf.gather_nd(boxes, ix)\n",
        "\n",
        "            # Box indices for crop_and_resize.\n",
        "            box_indices = tf.cast(ix[:, 0], tf.int32)\n",
        "\n",
        "            # Keep track of which box is mapped to which level\n",
        "            box_to_level.append(ix)\n",
        "\n",
        "            # Stop gradient propogation to ROI proposals\n",
        "            level_boxes = tf.stop_gradient(level_boxes)\n",
        "            box_indices = tf.stop_gradient(box_indices)\n",
        "\n",
        "            pooled.append(tf.image.crop_and_resize(\n",
        "                feature_maps[i], level_boxes, box_indices, self.pool_shape,\n",
        "                method=\"bilinear\"))\n",
        "\n",
        "        # Pack pooled features into one tensor\n",
        "        pooled = tf.concat(pooled, axis=0)\n",
        "\n",
        "        # Pack box_to_level mapping into one array and add another\n",
        "        # column representing the order of pooled boxes\n",
        "        box_to_level = tf.concat(box_to_level, axis=0)\n",
        "        box_range = tf.expand_dims(tf.range(tf.shape(box_to_level)[0]), 1)\n",
        "        box_to_level = tf.concat([tf.cast(box_to_level, tf.int32), box_range],\n",
        "                                 axis=1)\n",
        "\n",
        "        # Rearrange pooled features to match the order of the original boxes\n",
        "        # Sort box_to_level by batch then box index\n",
        "        # TF doesn't have a way to sort by two columns, so merge them and sort.\n",
        "        sorting_tensor = box_to_level[:, 0] * 100000 + box_to_level[:, 1]\n",
        "        ix = tf.nn.top_k(sorting_tensor, k=tf.shape(\n",
        "            box_to_level)[0]).indices[::-1]\n",
        "        ix = tf.gather(box_to_level[:, 2], ix)\n",
        "        pooled = tf.gather(pooled, ix)\n",
        "\n",
        "        # Re-add the batch dimension\n",
        "        shape = tf.concat([tf.shape(boxes)[:2], tf.shape(pooled)[1:]], axis=0)\n",
        "        pooled = tf.reshape(pooled, shape)\n",
        "        return pooled\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0][:2] + self.pool_shape + (input_shape[2][-1], )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH9M3nzPWhe9",
        "colab_type": "text"
      },
      "source": [
        "# **MRCNN CLASSIFIER AND MASK HEADS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMr8Qv7tWr_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''Classifier graph to finally classify the object classes and to find bbox regression for them\n",
        "Author - Asad Tanveer '''\n",
        "def fpn_classifier_graph(rois, feature_maps,pool_size, num_classes, train_bn=True,fc_layers_size=1024):\n",
        "    # ROI Pooling\n",
        "    # Shape: [batch, num_rois, POOL_SIZE, POOL_SIZE, channels]\n",
        "    # x = roi_allign([rois]+feature_maps)\n",
        "    x = ROIAlign([pool_size,pool_size])([rois] + mrcnn_feature_maps)\n",
        "    # x = PyramidROIAlign([pool_size, pool_size],\n",
        "    #                     name=\"roi_align_classifier\")([rois, image] + feature_maps)\n",
        "    # Two 1024 FC layers (implemented with Conv2D for consistency)\n",
        "    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (pool_size, pool_size), padding=\"valid\"),\n",
        "                           name=\"mrcnn_class_conv1\")(x)\n",
        "    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn1')(x, training=train_bn)\n",
        "    x = KL.Activation('relu')(x)\n",
        "    x = KL.TimeDistributed(KL.Conv2D(fc_layers_size, (1, 1)),\n",
        "                           name=\"mrcnn_class_conv2\")(x)\n",
        "    x = KL.TimeDistributed(BatchNorm(), name='mrcnn_class_bn2')(x, training=train_bn)\n",
        "    x = KL.Activation('relu')(x)\n",
        "\n",
        "    shared = KL.Lambda(lambda x: K.squeeze(K.squeeze(x, 3), 2),\n",
        "                       name=\"pool_squeeze\")(x)\n",
        "\n",
        "    # Classifier head\n",
        "    mrcnn_class_logits = KL.TimeDistributed(KL.Dense(num_classes),\n",
        "                                            name='mrcnn_class_logits')(shared)\n",
        "    mrcnn_probs = KL.TimeDistributed(KL.Activation(\"softmax\"),\n",
        "                                     name=\"mrcnn_class\")(mrcnn_class_logits)\n",
        "\n",
        "    # BBox head\n",
        "    # [batch, num_rois, NUM_CLASSES * (dy, dx, log(dh), log(dw))]\n",
        "    x = KL.TimeDistributed(KL.Dense(num_classes * 4, activation='linear'),\n",
        "                           name='mrcnn_bbox_fc')(shared)\n",
        "    # Reshape to [batch, num_rois, NUM_CLASSES, (dy, dx, log(dh), log(dw))]\n",
        "    s = K.int_shape(x)\n",
        "    # mrcnn_bbox=x\n",
        "    mrcnn_bbox = KL.Reshape((s[1], num_classes, 4), name=\"mrcnn_bbox\")(x)\n",
        "    # mrcnn_bbox=x\n",
        "    return mrcnn_class_logits, mrcnn_probs, mrcnn_bbox\n",
        "\n",
        "'''Mask head to generate a mask for the identified class with the bbox\n",
        "Author - Joseph Arul Raj '''\n",
        "def build_fpn_mask_graph(rois, feature_maps,\n",
        "                         pool_size, num_classes, train_bn=True):\n",
        "    # ROI Pooling\n",
        "    # Shape: [batch, num_rois, MASK_POOL_SIZE, MASK_POOL_SIZE, channels]\n",
        "    # x = ROIAlign([pool_size,rois,feature_maps])\n",
        "    x = ROIAlign([pool_size,pool_size])([rois] + mrcnn_feature_maps)\n",
        "    # PyramidROIAlign([pool_size, pool_size],\n",
        "    #                     name=\"roi_align_mask\")([rois, image] + feature_maps)\n",
        "\n",
        "    # Conv layers\n",
        "    x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"),\n",
        "                           name=\"mrcnn_mask_conv1\")(x)\n",
        "    x = KL.TimeDistributed(BatchNorm(),\n",
        "                           name='mrcnn_mask_bn1')(x, training=train_bn)\n",
        "    x = KL.Activation('relu')(x)\n",
        "\n",
        "    x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"),\n",
        "                           name=\"mrcnn_mask_conv2\")(x)\n",
        "    x = KL.TimeDistributed(BatchNorm(),\n",
        "                           name='mrcnn_mask_bn2')(x, training=train_bn)\n",
        "    x = KL.Activation('relu')(x)\n",
        "\n",
        "    x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"),\n",
        "                           name=\"mrcnn_mask_conv3\")(x)\n",
        "    x = KL.TimeDistributed(BatchNorm(),\n",
        "                           name='mrcnn_mask_bn3')(x, training=train_bn)\n",
        "    x = KL.Activation('relu')(x)\n",
        "\n",
        "    x = KL.TimeDistributed(KL.Conv2D(256, (3, 3), padding=\"same\"),\n",
        "                           name=\"mrcnn_mask_conv4\")(x)\n",
        "    x = KL.TimeDistributed(BatchNorm(),\n",
        "                           name='mrcnn_mask_bn4')(x, training=train_bn)\n",
        "    x = KL.Activation('relu')(x)\n",
        "\n",
        "    x = KL.TimeDistributed(KL.Conv2DTranspose(256, (2, 2), strides=2, activation=\"relu\"),\n",
        "                           name=\"mrcnn_mask_deconv\")(x)\n",
        "    x = KL.TimeDistributed(KL.Conv2D(num_classes, (1, 1), strides=1, activation=\"sigmoid\"),\n",
        "                           name=\"mrcnn_mask\")(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWxX3LFFXStl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifier head\n",
        "\n",
        "mrcnn_class_logits, mrcnn_class, mrcnn_bbox= fpn_classifier_graph(rois, mrcnn_feature_maps,7, 2)\n",
        "\n",
        "# Mask head\n",
        "\n",
        "mrcnn_mask = build_fpn_mask_graph(rois, mrcnn_feature_maps,14,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcpynsudYwnk",
        "colab_type": "text"
      },
      "source": [
        "## **GENERATE DATA FOR TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgdKYomfYt_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Since our model has to scale down the image to six times during training,we need the inputs which can be divided by 2 six times .\n",
        "so we need to resize the inputs \n",
        "Author - Asad Tanveer'''\n",
        "\n",
        "def resize(image, output_shape, order=1, mode='constant', cval=0, clip=True,\n",
        "           preserve_range=False, anti_aliasing=False, anti_aliasing_sigma=None):\n",
        "\n",
        "    if LooseVersion(skimage.__version__) >= LooseVersion(\"0.14\"):\n",
        "        # New in 0.14: anti_aliasing. Default it to False for backward\n",
        "        # compatibility with skimage 0.13.\n",
        "        return skimage.transform.resize(\n",
        "            image, output_shape,\n",
        "            order=order, mode=mode, cval=cval, clip=clip,\n",
        "            preserve_range=preserve_range, anti_aliasing=anti_aliasing,\n",
        "            anti_aliasing_sigma=anti_aliasing_sigma)\n",
        "    else:\n",
        "        return skimage.transform.resize(\n",
        "            image, output_shape,\n",
        "            order=order, mode=mode, cval=cval, clip=clip,\n",
        "            preserve_range=preserve_range)\n",
        "\n",
        "\n",
        "def resize_image(image, min_dim=800, max_dim=1024, min_scale=None, mode=\"square\"):\n",
        "    # Keep track of image dtype and return results in the same dtype\n",
        "    image_dtype = image.dtype\n",
        "\n",
        "    # Scale?\n",
        "    if min_dim:\n",
        "        # Scale up but not down\n",
        "        scale = max(1, min_dim / min(h, w))\n",
        "    if min_scale and scale < min_scale:\n",
        "        scale = min_scale\n",
        "\n",
        "    # Does it exceed max dim?\n",
        "    if max_dim and mode == \"square\":\n",
        "        image_max = max(h, w)\n",
        "        if round(image_max * scale) > max_dim:\n",
        "            scale = max_dim / image_max\n",
        "\n",
        "    # Resize image using bilinear interpolation\n",
        "    if scale != 1:\n",
        "        image = resize(image, (round(h * scale), round(w * scale)),\n",
        "                       preserve_range=True)\n",
        "\n",
        "    # Need padding or cropping?\n",
        "    if mode == \"square\":\n",
        "        # Get new height and width\n",
        "        h, w = image.shape[:2]\n",
        "        top_pad = (max_dim - h) // 2\n",
        "        bottom_pad = max_dim - h - top_pad\n",
        "        left_pad = (max_dim - w) // 2\n",
        "        right_pad = max_dim - w - left_pad\n",
        "        padding = [(top_pad, bottom_pad), (left_pad, right_pad), (0, 0)]\n",
        "        image = np.pad(image, padding, mode='constant', constant_values=0)\n",
        "        window = (top_pad, left_pad, h + top_pad, w + left_pad)\n",
        "\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Mode {} not supported\".format(mode))\n",
        "    return image.astype(image_dtype), window, scale, padding\n",
        "\n",
        "\n",
        "def resize_mask(mask, scale, padding):\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        mask = scipy.ndimage.zoom(mask, zoom=[scale, scale, 1], order=0)\n",
        "    mask = np.pad(mask, padding, mode='constant', constant_values=0)\n",
        "    return mask\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjRfHF8hbGfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''A python generator function to generate the training data\n",
        "author - Joseph Arul Raj '''\n",
        "\n",
        "def mold_image(images):\n",
        "    \"\"\"normalize the pixel values\n",
        "    \"\"\"\n",
        "    MEAN_PIXEL = np.array([123.7, 116.8, 103.9])\n",
        "    return images.astype(np.float32) - MEAN_PIXEL\n",
        "def data_generator(batch_size=2):\n",
        "    \n",
        "    b = 0  # batch item index\n",
        "    image_index = -1\n",
        "    # image_ids = np.copy(dataset.image_ids)\n",
        "    error_count = 0\n",
        "\n",
        "    # Anchors\n",
        "    # [anchor_count, (y1, x1, y2, x2)]\n",
        "    MAX_GT_INSTANCES =100\n",
        "    img_shape=[1024,1024,3]\n",
        "    feature_strides=[4, 8, 16, 32, 64]\n",
        "    backbone_shapes = compute_backbone_shapes(img_shape,feature_strides)\n",
        "    anchors=generate_pyramid_anchors((32, 64, 128, 256, 512),[0.5, 1, 2],backbone_shapes,feature_strides,1)\n",
        "    train=anno()\n",
        "    da=train.load_data(data_dir)\n",
        "    print('............')\n",
        "    masks=train.load_mask(da)\n",
        "    print('............')\n",
        "    # bboxes=train.extract_bboxes(masks)\n",
        "    print('............')\n",
        "    img_dir=data_dir + \"/img/\"\n",
        "    images=np.zeros([2,1080,1920,3])\n",
        "    for filename , i in zip(listdir(img_dir),range(2)):\n",
        "      im=img_dir+filename\n",
        "      print(i,filename)\n",
        "      images[i]=img_to_array(load_img(im,target_size=(1080,1920)))\n",
        "\n",
        "    info=train.image_info\n",
        "\n",
        "    # Keras requires a generator to run indefinitely.\n",
        "    while True:\n",
        "        print('Loop started....................................................................................',image_index)\n",
        "        try:\n",
        "            # Increment index to pick next image. Shuffle if at the start of an epoch.\n",
        "            print('started....')\n",
        "            image_index = (image_index + 1)\n",
        "\n",
        "            image=images[image_index]\n",
        "            image, window, scale, padding= resize_image(image,min_dim=800,min_scale=0,max_dim=1024,mode='square')\n",
        "            print('image',image.shape)\n",
        "            gt_masks=masks[image_index]\n",
        "            gt_masks= resize_mask(gt_masks, scale, padding, crop)\n",
        "            print('masks',gt_masks.shape)\n",
        "            gt_boxes=train.extract_bboxes(gt_masks)\n",
        "            print('bbox',gt_boxes.shape)\n",
        "\n",
        "            # Init batch arrays\n",
        "            if b == 0:\n",
        "                print('batching...')\n",
        "                batch_images = np.zeros(\n",
        "                    (batch_size,) + image.shape, dtype=np.float32)\n",
        "                print('im')\n",
        "                # batch_gt_class_ids = np.zeros(\n",
        "                #     (batch_size, MAX_GT_INSTANCES), dtype=np.int32)\n",
        "                batch_gt_boxes = np.zeros(\n",
        "                    (batch_size,MAX_GT_INSTANCES, 4), dtype=np.int32)\n",
        "                print('bb')\n",
        "                batch_gt_masks = np.zeros(\n",
        "                    (batch_size, gt_masks.shape[0], gt_masks.shape[1],\n",
        "                     MAX_GT_INSTANCES), dtype=gt_masks.dtype)\n",
        "    \n",
        "\n",
        "                print('batch created....')\n",
        "           # If more instances 1080than fits in the array, sub-sample from them.\n",
        "            print('gt instances present : ',gt_boxes.shape[0])\n",
        "            if gt_boxes.shape[0] >MAX_GT_INSTANCES:\n",
        "                ids = np.random.choice(\n",
        "                    np.arange(gt_boxes.shape[0]), MAX_GT_INSTANCES, replace=False)\n",
        "                print('ids',ids.shape)\n",
        "                print('...........',ids[1],ids[2])\n",
        "                # gt_class_ids = gt_class_ids[ids]\n",
        "                gt_boxes = gt_boxes[ids]\n",
        "                print('...........',ids[3],ids[5],gt_masks.shape)\n",
        "                gt_masks = gt_masks[:, :, ids]\n",
        "                print('instances<=100')\n",
        "            \n",
        "\n",
        "            # Add to batch\n",
        "\n",
        "            batch_images[b] = mold_image(image.astype(np.float32))\n",
        "            print('batch image')\n",
        "            # batch_gt_class_ids[b, :gt_class_ids.shape[0]] = gt_class_ids\n",
        "            batch_gt_boxes[b, :gt_boxes.shape[0]] = gt_boxes\n",
        "            print('batch bbox')\n",
        "            batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks\n",
        "            print('batch mask')\n",
        "\n",
        "\n",
        "            b += 1\n",
        "\n",
        "            # Batch full?\n",
        "            if b >= batch_size:\n",
        "                print('Batch full')\n",
        "                inputs = [batch_images, batch_gt_boxes, batch_gt_masks]\n",
        "\n",
        "                yield inputs\n",
        "\n",
        "                # start a new batch\n",
        "                b = 0\n",
        "        except (GeneratorExit, KeyboardInterrupt):\n",
        "            raise\n",
        "        except:\n",
        "            # Log it and skip the image\n",
        "            logging.exception(\"Error processing image {}\".format(info[image_index]))\n",
        "            error_count += 1\n",
        "            if error_count > 5:\n",
        "                raise\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYdmIQYEXb9K",
        "colab_type": "text"
      },
      "source": [
        "# **MODEL BUILDING AND TRAINING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14coX9HgX49n",
        "colab_type": "text"
      },
      "source": [
        "MODEL **BUILDING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9qI3Ro6XmW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_rois = KL.Lambda(lambda x: x * 1, name=\"output_rois\")(rois)\n",
        "\n",
        "inputs = [input_image, input_gt_boxes, input_gt_masks]\n",
        "outputs = [rpn_class_logits, rpn_class, rpn_bbox, mrcnn_class_logits, mrcnn_class, mrcnn_bbox,mrcnn_mask, rpn_rois, output_rois]\n",
        "model = KM.Model(inputs, outputs, name='mask_rcnn')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vc_PRrZX8B3",
        "colab_type": "text"
      },
      "source": [
        "**COMPILE** **IT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lEKMydeX_YB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMvr1au4X_5X",
        "colab_type": "text"
      },
      "source": [
        "**TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvg83deCYGj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the training data\n",
        "\n",
        "train_datas=data_generator()\n",
        "\n",
        "# Fit the data\n",
        "\n",
        "model.fit(train_datas,steps_per_epoch=1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}